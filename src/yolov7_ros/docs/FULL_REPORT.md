# 基於深度學習語義分割與模糊邏輯控制的  
# 自主移動機器人視覺避障系統

---

**Vision-based Obstacle Avoidance System for Autonomous Mobile Robots  
Using Deep Learning Semantic Segmentation and Fuzzy Logic Control**

---

> **報告日期**：2026 年 1 月 9 日  
> **實驗平台**：NVIDIA Jetson Orin NX + ROS1 Noetic  
> **系統版本**：v4.2

---

## 目錄

1. [研究背景與動機](#1-研究背景與動機)
2. [相關研究](#2-相關研究)
3. [系統架構設計](#3-系統架構設計)
4. [視覺感知模組](#4-視覺感知模組)
5. [模糊控制決策模組](#5-模糊控制決策模組)
6. [語音人機介面](#6-語音人機介面)
7. [系統整合與控制流程](#7-系統整合與控制流程)
8. [實驗設計](#8-實驗設計)
9. [實驗結果與分析](#9-實驗結果與分析)
10. [結論與未來展望](#10-結論與未來展望)
11. [參考文獻](#11-參考文獻)

---

## 1. 研究背景與動機

### 1.1 研究背景

隨著人工智慧與機器人技術的快速發展，自主移動機器人（Autonomous Mobile Robot, AMR）在醫療照護、倉儲物流、居家服務等領域的應用日益廣泛。在這些應用場景中，機器人必須具備可靠的**環境感知**與**自主避障**能力，以確保安全且高效的導航。

傳統的避障方法主要依賴**光達（LiDAR）**感測器進行測距與障礙物偵測。然而，LiDAR 感測器存在以下限制：

1. **垂直視野受限**：2D LiDAR 僅能偵測固定高度平面上的障礙物，對於低於或高於掃描平面的障礙物無法有效偵測。
2. **材質敏感性**：對於透明、反光或吸光材質（如玻璃、鏡面、黑色物體）的偵測效果較差。
3. **成本考量**：高精度 3D LiDAR 成本較高，不利於大規模商業應用。

### 1.2 研究動機

基於上述背景，本研究提出一種**基於視覺的避障方法**，利用深度學習語義分割技術識別可行駛道路區域，並採用模糊邏輯控制器進行運動決策。此方法具有以下優勢：

1. **豐富的感知資訊**：相機可獲取場景的顏色、紋理、形狀等多維度資訊
2. **成本效益**：相機成本遠低於 LiDAR
3. **互補性**：可偵測 LiDAR 盲區內的障礙物（如低矮物體）

### 1.3 研究目標

本研究的主要目標為：

1. 建立基於 YOLO11 語義分割的即時道路區域偵測系統
2. 設計四輸入二輸出模糊控制器實現平滑的避障運動
3. 開發語音人機介面實現自然語言指令控制
4. 驗證系統在實際環境中的避障性能

---

## 2. 相關研究

### 2.1 視覺避障技術

視覺避障技術可分為以下類別：

| 方法類別 | 代表方法 | 優點 | 缺點 |
|----------|----------|------|------|
| 傳統影像處理 | 邊緣偵測、色彩分割 | 計算量小 | 對光照敏感 |
| 深度學習目標偵測 | YOLO, SSD, Faster R-CNN | 可識別多類別物體 | 無法直接得到可行駛區域 |
| 深度學習語義分割 | FCN, U-Net, DeepLab | 像素級分類 | 計算量較大 |
| 實例分割 | Mask R-CNN, YOLACT | 區分個別物體 | 需要大量標註資料 |

本研究採用 **YOLO11n-seg** 模型進行語義分割，該模型具有以下特點：
- 輕量化架構適合邊緣運算平台
- 支援 TensorRT 加速推論
- 可同時輸出偵測框與分割遮罩

### 2.2 模糊邏輯控制

模糊邏輯控制（Fuzzy Logic Control）由 Zadeh 於 1965 年提出，適用於處理不確定性與非線性系統。在機器人導航領域，模糊控制具有以下優勢：

1. **無需精確數學模型**：透過語言規則描述控制策略
2. **處理不確定性**：能容忍感測器雜訊與環境變化
3. **平滑輸出**：避免傳統 if-else 控制的突變問題

相關研究包括：
- Saffiotti (1997) 提出基於行為的模糊控制器用於移動機器人導航
- Hagras (2004) 設計階層式模糊控制系統提升複雜環境適應能力

### 2.3 鳥瞰圖變換

鳥瞰圖（Bird's Eye View, BEV）變換將透視影像轉換為俯視視角，便於進行距離估測與路徑規劃。常見方法包括：

1. **逆透視映射（IPM）**：假設地面為平坦，透過相機內外參數進行幾何變換
2. **深度學習方法**：如 BEVFormer，從多視角影像學習 BEV 表示

本研究採用**幾何投影法**，基於相機俯仰角與安裝高度計算像素到地面座標的映射。

---

## 3. 系統架構設計

### 3.1 整體架構

本系統採用**分層式模組化架構**，建構於 ROS1（Robot Operating System）框架之上。系統由四個主要層級組成：

```
┌─────────────────────────────────────────────────────────────────────────────┐
│                         系統架構示意圖                                        │
│                                                                              │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                    應用層 (Application Layer)                          │  │
│   │                  語音人機介面 + 狀態監控                                 │  │
│   └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                         │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                    決策層 (Decision Layer)                             │  │
│   │              模糊邏輯控制器 + 動作規劃                                   │  │
│   └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                         │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                    感知層 (Perception Layer)                           │  │
│   │           YOLO11 語義分割 + BEV 座標轉換                                │  │
│   └──────────────────────────────────────────────────────────────────────┘  │
│                                    │                                         │
│   ┌──────────────────────────────────────────────────────────────────────┐  │
│   │                    執行層 (Execution Layer)                            │  │
│   │              差速驅動底盤 + 馬達控制                                     │  │
│   └──────────────────────────────────────────────────────────────────────┘  │
│                                                                              │
└─────────────────────────────────────────────────────────────────────────────┘
```

### 3.2 ROS 節點架構

系統由以下四個主要節點組成：

| 節點名稱 | 檔案 | 功能 |
|----------|------|------|
| `road_detection_node` | mod_predict_yolo11_trt.py | 道路分割與 BEV 轉換 |
| `fuzzy_controller_node` | mod_fuzzy_control4.py | 模糊推論與速度輸出 |
| `voice_command_node` | mod_voice_processing4.py | 語音辨識與指令解析 |
| `integrated_controller_node` | integrated_control4.py | 系統整合與狀態機管理 |

### 3.3 ROS Topic 通訊

```
road_detection_node                   fuzzy_controller_node
        │                                     │
        │ /road_info                          │ /fuzzy_cmd_vel
        │ (Float32MultiArray)                 │ (Twist)
        ▼                                     ▼
    ┌───────────────────────────────────────────────┐
    │           integrated_controller_node          │
    │                                               │
    │  ◄────── /voice_command_code (String) ───────│
    │                                               │
    │  ◄────── /avoidance_enabled (String) ────────│
    │                                               │
    └───────────────────────────────────────────────┘
                           │
                           │ /cmd_vel (Twist)
                           ▼
                 ┌─────────────────┐
                 │   機器人底盤    │
                 └─────────────────┘
```

### 3.4 硬體規格

| 項目 | 規格 |
|------|------|
| **運算平台** | NVIDIA Jetson Orin NX 16GB |
| **作業系統** | Ubuntu 20.04 LTS |
| **ROS 版本** | ROS1 Noetic |
| **相機** | CSI 相機，640×480 @ 30fps |
| **語音輸入** | 藍牙麥克風 |
| **機器人底盤** | 差速驅動，輪距 0.35m |

---

## 4. 視覺感知模組

### 4.1 道路語義分割

#### 4.1.1 模型選擇

本研究選用 **YOLO11n-seg** 作為道路分割模型，理由如下：

1. **輕量化**：n 版本參數量最小，適合邊緣運算
2. **高效能**：支援 TensorRT 加速，可達 15-20 fps
3. **端到端**：同時輸出偵測框與分割遮罩

#### 4.1.2 模型訓練

| 參數 | 設定值 |
|------|--------|
| 訓練資料集 | 自建走廊道路資料集（~500 張） |
| 輸入尺寸 | 640×640 |
| 訓練週期 | 100 epochs |
| 優化器 | AdamW |
| 學習率 | 0.01 |
| 資料增強 | 水平翻轉、亮度調整、縮放 |

#### 4.1.3 TensorRT 加速

為提升推論速度，將 PyTorch 模型轉換為 TensorRT 引擎：

```bash
yolo export model=best.pt format=engine device=0
```

轉換後推論速度提升約 2-3 倍。

### 4.2 鳥瞰圖座標轉換

#### 4.2.1 相機模型

假設相機以俯仰角 θ 安裝於高度 h 處，光軸指向前下方：

**相機參數：**
| 參數 | 符號 | 數值 |
|------|------|------|
| 安裝高度 | $h$ | 0.60 m |
| 俯仰角 | $\theta$ | 29° |
| 焦距 | $(f_x, f_y)$ | (457.1, 458.4) |
| 主點 | $(c_x, c_y)$ | (320, 240) |

#### 4.2.2 座標轉換公式

給定像素座標 $(u, v)$，計算地面座標 $(x_{ground}, y_{ground})$：

**步驟 1：計算像素偏角**
$$
\alpha = \arctan\left(\frac{u - c_x}{f_x}\right), \quad
\beta = \arctan\left(\frac{v - c_y}{f_y}\right)
$$

**步驟 2：計算總俯仰角**
$$
\theta_{total} = \theta + \beta
$$

**步驟 3：計算地面座標**
$$
y_{ground} = \frac{h}{\tan(\theta_{total})}, \quad
x_{ground} = \frac{h}{\sin(\theta_{total})} \cdot \tan(\alpha)
$$

#### 4.2.3 距離校正

為補償透視失真，應用二次多項式校正：
$$
y_{corrected} = a \cdot y_{raw}^2 + b \cdot y_{raw} + c
$$

校正參數透過實測數據擬合得到：
| 參數 | 數值 |
|------|------|
| $a$ | -0.0921 |
| $b$ | 1.0379 |
| $c$ | 0.1030 |

### 4.3 誤差變數計算

#### 4.3.1 雙參考點設計

為解耦前方距離與橫向誤差的計算，採用**分離式參考點**：

| 參考點 | 用途 | 計算方式 |
|--------|------|----------|
| **前方參考點** | 計算 $e_d$ | 畫面中心垂直線與遮罩最高交點 |
| **橫向參考點** | 計算 $e_l$ | 固定 y 座標處的遮罩中心 |

#### 4.3.2 誤差定義

| 變數 | 定義 | 範圍 |
|------|------|------|
| $e_d$ | 前方可視距離 - 停止距離 | [0, 2.08] m |
| $\dot{e_d}$ | $e_d$ 的時間導數 | [-2.08, 2.08] m/s |
| $e_l$ | 橫向偏移量（右為正） | [-0.5, 0.5] m |
| $\dot{e_l}$ | $e_l$ 的時間導數 | [-1.0, 1.0] m/s |

---

## 5. 模糊控制決策模組

### 5.1 模糊控制器架構

本系統採用**四輸入二輸出 Mamdani 型模糊控制器**：

| 類型 | 變數 | 個數 |
|------|------|------|
| 輸入 | $e_d$, $\dot{e_d}$, $e_l$, $\dot{e_l}$ | 4 |
| 輸出 | $v$ (線速度), $\omega$ (角速度) | 2 |
| 規則數 | $5^4$ | 625 |

### 5.2 隸屬函數設計

#### 5.2.1 輸入變數

**前方距離誤差 $e_d$：**

| 語言變數 | 縮寫 | 參數 (a, b, c) | 物理意義 |
|----------|------|----------------|----------|
| Very Near | VN | (0, 0, 0.52) | 危險接近 |
| Near | N | (0, 0.52, 1.04) | 接近停止距離 |
| Medium | M | (0.52, 1.04, 1.56) | 正常範圍 |
| Far | F | (1.04, 1.56, 2.08) | 安全距離 |
| Very Far | VF | (1.56, 2.08, 2.08) | 視野良好 |

**橫向誤差 $e_l$（非對稱死區設計）：**

| 語言變數 | 參數 (a, b, c) | 設計特點 |
|----------|----------------|----------|
| NB | (-0.5, -0.5, -0.15) | 左偏大 |
| NS | (-0.5, -0.15, -0.05) | 死區邊界在 -0.05m |
| ZO | (-0.08, 0, 0.08) | 擴大死區 ±0.08m |
| PS | (0.05, 0.15, 0.5) | 死區邊界在 0.05m |
| PB | (0.15, 0.5, 0.5) | 右偏大 |

> **死區設計理由**：在 [-0.05, 0.05] 區間內僅 ZO 被激發，避免微小擾動觸發不必要的轉向修正。

#### 5.2.2 輸出變數（Singleton 型）

**線速度 $v$：**
| 語言變數 | 數值 (m/s) |
|----------|-----------|
| Stop (S) | 0.00 |
| Very Slow (VS) | 0.18 |
| Slow (SL) | 0.32 |
| Medium (M) | 0.45 |
| Fast (F) | 0.55 |

**角速度 $\omega$：**
| 語言變數 | 數值 (rad/s) |
|----------|-------------|
| NB | -1.4 |
| NS | -0.5 |
| ZO | 0.0 |
| PS | 0.5 |
| PB | 1.4 |

### 5.3 模糊規則庫

#### 5.3.1 規則產生策略

採用**分階段合成法**：

**階段 1：基準規則**
- 線速度基準表：$v_{base} = f(e_d, \dot{e_d})$
- 角速度基準表：$\omega_{base} = f(e_l, \dot{e_l})$

**階段 2：交互修正**
- 線速度修正：$e_l$ 偏大時降速
- 角速度修正：$e_d$ 很近時減少角速度

#### 5.3.2 線速度基準規則表

| $e_d$ ╲ $\dot{e_d}$ | NB | NS | ZO | PS | PB |
|:---:|:---:|:---:|:---:|:---:|:---:|
| **VN** | S | S | S | VS | VS |
| **N** | S | VS | VS | SL | SL |
| **M** | SL | SL | SL | M | F |
| **F** | SL | M | M | F | F |
| **VF** | M | M | F | F | F |

#### 5.3.3 角速度基準規則表

| $e_l$ ╲ $\dot{e_l}$ | NB | NS | ZO | PS | PB |
|:---:|:---:|:---:|:---:|:---:|:---:|
| **NB** | PB | PB | PB | PS | PS |
| **NS** | PB | PB | PS | PS | PS |
| **ZO** | PS | PS | ZO | NS | NS |
| **PS** | NS | NS | NS | NB | NB |
| **PB** | NS | NS | NB | NB | NB |

#### 5.3.4 規則分佈統計

| 線速度 | 數量 | 比例 | | 角速度 | 數量 | 比例 |
|:---:|:---:|:---:|:---:|:---:|:---:|:---:|
| S | 240 | 38.4% | | NB | 100 | 16.0% |
| VS | 130 | 20.8% | | NS | 165 | 26.4% |
| SL | 140 | 22.4% | | ZO | 95 | 15.2% |
| M | 85 | 13.6% | | PS | 165 | 26.4% |
| F | 30 | 4.8% | | PB | 100 | 16.0% |

> 角速度左轉規則 (265 條) = 右轉規則 (265 條)，滿足對稱性。

### 5.4 模糊推論方法

#### 5.4.1 模糊化

採用三角形/梯形隸屬函數：
$$
\mu(x) = \begin{cases}
0 & x < a \\
\frac{x-a}{b-a} & a \leq x \leq b \\
\frac{c-x}{c-b} & b \leq x \leq c \\
0 & x > c
\end{cases}
$$

#### 5.4.2 規則評估

採用 **AND 運算（最小值）** 計算規則激發強度：
$$
w_i = \min(\mu_{e_d}^{(i)}, \mu_{\dot{e_d}}^{(i)}, \mu_{e_l}^{(i)}, \mu_{\dot{e_l}}^{(i)})
$$

#### 5.4.3 解模糊化

採用**加權平均法**：
$$
y = \frac{\sum_{i=1}^{N} w_i \cdot c_i}{\sum_{i=1}^{N} w_i}
$$

其中 $c_i$ 為第 $i$ 條規則的輸出 Singleton 值。

### 5.5 後處理

#### 5.5.1 範圍限制
```python
v = clip(v, 0.0, 0.55)
omega = clip(omega, -1.4, 1.4)
```

#### 5.5.2 低通濾波

採用指數移動平均（EMA）消除高頻抖動：
$$
y_t = \alpha \cdot x_t + (1 - \alpha) \cdot y_{t-1}
$$

| 參數 | 數值 | 效果 |
|------|------|------|
| $\alpha_v$ | 0.2 | 強濾波，消除線速度猶豫 |
| $\alpha_\omega$ | 0.3 | 中度濾波，保留轉向靈敏度 |

---

## 6. 語音人機介面

### 6.1 語音處理流程

```
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│ 藍牙麥克風 │ ──▶│ VAD 錄音 │ ──▶│ 噪音濾除 │ ──▶│ MP3 編碼 │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
                                                      │
                                                      ▼
┌──────────┐    ┌──────────┐    ┌──────────┐    ┌──────────┐
│ ROS 發布 │ ◀──│ 指令解析 │ ◀──│ 語音辨識 │ ◀──│ API 傳送 │
└──────────┘    └──────────┘    └──────────┘    └──────────┘
```

### 6.2 VAD 智能錄音

採用 RMS（均方根）音量閾值判斷語音活動：

| 參數 | 數值 | 說明 |
|------|------|------|
| SILENCE_THRESHOLD | 1000 | 靜音閾值（RMS） |
| SPEECH_START_THRESHOLD | 5 | 開始錄音所需連續幀數 |
| SPEECH_END_THRESHOLD | 10 | 結束錄音所需連續靜音幀數 |
| MAX_RECORDING_SECONDS | 5 | 最大錄音時間 |

### 6.3 噪音濾除

使用 `noisereduce` 函式庫進行頻譜減法降噪：
1. 取錄音前 0.5 秒作為純噪聲樣本
2. 估算噪聲頻譜輪廓
3. 從語音訊號中減去噪聲成分

### 6.4 語音指令編碼

| 編碼 | 語音指令 | 機器人動作 |
|------|----------|------------|
| 11000 | 開始巡邏 / 向前進 | 啟動模糊避障 |
| 10000 | 暫停 / 待命 | 停止並等待指令 |
| 21000 | 觀察環境 | 左右轉頭 90° |
| 13000 | 左轉 | 原地左轉 90° |
| 14000 | 右轉 | 原地右轉 90° |
| 15000 | 左轉向前 | 左轉 90° 後啟動避障 |
| 16000 | 右轉向前 | 右轉 90° 後啟動避障 |

---

## 7. 系統整合與控制流程

### 7.1 狀態機設計

```
                    ┌─────────────┐
                    │   待命狀態   │
                    └──────┬──────┘
                           │ 語音「開始」或 鍵盤 S
                           ▼
                    ┌─────────────┐
                    │  任務啟動   │
                    └──────┬──────┘
                           │ avoid_obstacle = True
                           ▼
         ┌────────────────────────────────┐
         │                                │
         ▼                                │
  ┌─────────────┐    語音轉彎指令    ┌────┴────┐
  │  模糊避障   │ ─────────────────▶ │ 執行動作 │
  └──────┬──────┘                    └────┬────┘
         │ ◀─────── 動作完成 ─────────────┘
         │
         │ 語音「暫停」
         ▼
  ┌─────────────┐
  │  暫停狀態   │
  └─────────────┘
```

### 7.2 控制優先級

1. **語音指令優先**：執行中的語音動作不可被模糊控制覆蓋
2. **鍵盤連動**：語音與鍵盤控制雙向同步
3. **安全超時**：25 秒無回應自動恢復模糊控制

### 7.3 模糊控制主迴圈

```python
# 簡化虛擬碼
while not rospy.is_shutdown():
    if paused:
        pass  # 暫停狀態
    elif is_executing_voice_cmd:
        process_current_command()  # 執行語音動作
    elif avoidance_enabled and road_detected:
        cmd_vel_pub.publish(fuzzy_cmd)  # 模糊控制
    else:
        stop_robot()  # 停止
    rate.sleep()
```

---

## 8. 實驗設計

### 8.1 實驗環境

| 項目 | 規格 |
|------|------|
| **場地** | 室內走廊 |
| **走廊寬度** | 約 1.5 m |
| **障礙物** | 3 個固定位置障礙箱 |
| **路徑長度** | 約 7 m |
| **實驗次數** | 5 次 |

### 8.2 障礙物配置

```
     起點
       │
       ▼
  ┌─────────┐
  │         │
  │  ████   │  ← 障礙物 1（左側）
  │         │
  ├─────────┤
  │         │
  │    ████ │  ← 障礙物 2（右側）
  │         │
  ├─────────┤
  │         │
  │  ████   │  ← 障礙物 3（左側）
  │         │
  └─────────┘
       ▼
     終點
```

### 8.3 評估指標

#### 8.3.1 幾何軌跡準確性

| 指標 | 定義 |
|------|------|
| **ATE RMSE** | 絕對軌跡誤差均方根 |
| **Hausdorff Distance** | 最大點對點距離（最壞偏差） |
| **Fréchet Distance** | 曲線形狀相似度 |

#### 8.3.2 導航效率

| 指標 | 定義 |
|------|------|
| **SPL** | Success weighted by Path Length |
| **Success Rate** | 成功到達終點比例 |
| **Travel Time** | 導航總時間 |
| **Path Length** | 實際行走路徑長度 |

**SPL 計算公式：**
$$
SPL = \frac{1}{N} \sum_{i=1}^{N} S_i \cdot \frac{L_i^*}{\max(L_i, L_i^*)}
$$

其中 $S_i$ 為成功指標，$L_i^*$ 為最短路徑長度，$L_i$ 為實際路徑長度。

#### 8.3.3 運動平滑度

| 指標 | 定義 |
|------|------|
| **Average Jerk** | 加速度變化率均值 |
| **Curvature** | 路徑曲率 |
| **Total Turn** | 累積轉彎角度 |

#### 8.3.4 動態安全性

| 指標 | 定義 |
|------|------|
| **Minimum Distance** | 與障礙物最小距離 |
| **Collision Probability** | 碰撞機率 |

### 8.4 對照實驗

為驗證視覺方法對 LiDAR 的互補性，設計以下對照實驗：

**場景**：降低第二障礙物高度，使其低於 2D LiDAR 掃描平面

| 方法 | 感測器 | 預期結果 |
|------|--------|----------|
| LiDAR 方法 | 2D LiDAR | 無法偵測低矮障礙物 |
| 視覺模糊方法 | 相機 | 透過語義分割識別 |

---

## 9. 實驗結果與分析

### 9.1 視覺模糊控制性能

#### 9.1.1 定量結果

| 指標類別 | 指標 | 數值 | 評價 |
|----------|------|------|------|
| **幾何準確性** | ATE RMSE | 0.1873 ± 0.0456 m | 良好 |
| | Hausdorff Distance | 0.3594 m | 可接受 |
| | Fréchet Distance | 0.3594 m | 可接受 |
| **導航效率** | SPL | **0.9457 ± 0.0132** | ⭐ 優秀 |
| | Success Rate | **100%** | ⭐ 完美 |
| | Travel Time | 20.11 s | - |
| | Path Length | 7.18 m | - |
| **運動平滑度** | Average Jerk | 0.3482 m/s³ | 平滑 |
| | Curvature | 0.5337 | 正常 |
| | Total Turn | 223.01° | - |
| **動態安全性** | Minimum Distance | 0.3379 m | 安全 |
| | Collision Probability | **0%** | ⭐ 完美 |

#### 9.1.2 軌跡分析

本系統成功完成所有 5 次實驗，機器人能夠：
1. 準確識別道路可行區域
2. 及時偵測並繞過障礙物
3. 維持平滑的運動軌跡

### 9.2 與 LiDAR 方法比較

#### 9.2.1 低高度障礙物場景結果

| 項目 | LiDAR 方法 | 視覺模糊方法 |
|------|------------|--------------|
| **結果** | ❌ 碰撞失敗 | ✅ 成功避障 |
| 失敗位置 | 約 3.8 m 處 | - |
| Travel Time | - | 21.84 ± 3.68 s |
| Path Length | - | 7.21 ± 0.12 m |
| Average Jerk | - | 0.3549 m/s³ |
| Curvature | - | 0.6527 |
| Total Turn | - | 293.92° |
| Minimum Distance | - | 0.3644 m |

#### 9.2.2 分析與討論

**LiDAR 失敗原因：**
- 2D LiDAR 掃描平面固定於某一高度
- 當障礙物高度低於掃描平面時，雷射光束越過障礙物上方
- 無法返回有效障礙物訊號，導致碰撞

**視覺方法成功原因：**
- 相機具有完整的 2D 視野
- 語義分割識別道路區域，非道路區域視為障礙
- 不受障礙物高度限制

### 9.3 模糊控制器分析

#### 9.3.1 規則激發分析

在典型避障場景中，觀察到以下規則激發模式：

1. **直線行駛**：$e_l \approx 0$, ZO 規則主導，$\omega \approx 0$
2. **接近障礙**：$e_d$ 減小，線速度由 F → M → SL → VS
3. **繞行障礙**：$e_l \neq 0$，角速度規則激發轉向
4. **回歸中心**：$e_l$ 減小，角速度逐漸歸零

#### 9.3.2 低通濾波效果

| 狀況 | 無濾波 | 有濾波 |
|------|--------|--------|
| 線速度抖動 | 明顯 | 平滑 |
| 轉向擺動 | 頻繁 | 抑制 |
| 整體軌跡 | 鋸齒狀 | 平滑曲線 |

---

## 10. 結論與未來展望

### 10.1 研究成果

本研究成功開發了一套**基於深度學習語義分割與模糊邏輯控制**的視覺避障系統，主要貢獻如下：

1. **視覺感知**：採用 YOLO11n-seg 實現即時道路分割，配合 BEV 座標轉換提供準確的距離估測
2. **模糊控制**：設計 625 條規則的四輸入模糊控制器，實現平滑的避障運動
3. **人機介面**：整合語音辨識技術，提供自然語言指令控制
4. **系統整合**：建構完整的 ROS 節點架構，實現模組化與可擴展性

### 10.2 實驗驗證

實驗結果顯示：
- **避障成功率**：100%
- **路徑效率 (SPL)**：0.9457
- **碰撞機率**：0%
- **與 LiDAR 比較**：在低高度障礙物場景中展現優勢

### 10.3 研究限制

1. **計算資源**：TensorRT 加速後仍約 15-20 fps，低於 LiDAR 掃描頻率
2. **光照敏感**：相機在極端光照條件下可能失效
3. **動態障礙**：目前僅針對靜態障礙物設計

### 10.4 未來展望

1. **多感測器融合**：結合 LiDAR 與視覺的互補優勢
2. **動態障礙追蹤**：整合目標追蹤演算法處理移動障礙
3. **自適應模糊控制**：基於強化學習自動優化規則參數
4. **端對端語音**：部署離線語音辨識模型減少延遲

---

## 11. 參考文獻

[1] Jocher, G., et al. "YOLO by Ultralytics." GitHub, 2023.

[2] Zadeh, L. A. "Fuzzy sets." Information and control, 8(3), 338-353, 1965.

[3] Saffiotti, A. "The uses of fuzzy logic in autonomous robot navigation." Soft computing, 1(4), 180-197, 1997.

[4] Anderson, S. J., et al. "Evaluation of autonomous mobile robot navigation algorithms." Robotics and Autonomous Systems, 2010.

[5] Berger, C., et al. "Path planning for autonomous ground vehicles: A survey." IEEE Transactions on Intelligent Transportation Systems, 2014.

---

## 附錄 A：系統啟動指令

```bash
# 啟動完整系統
roslaunch yolov7_ros integrated_system.launch

# 測試模式（語音用鍵盤模擬）
roslaunch yolov7_ros integrated_system.launch test_mode:=true
```

## 附錄 B：關鍵參數配置

| 參數 | 檔案 | 預設值 |
|------|------|--------|
| stop_distance_m | camera_config.yaml | 1.04 m |
| pitch_deg | camera_config.yaml | 29° |
| height_m | camera_config.yaml | 0.60 m |
| alpha_v | mod_fuzzy_control4.py | 0.2 |
| alpha_omega | mod_fuzzy_control4.py | 0.3 |
| lateral_ref_distance | mod_predict_yolo11_trt.py | 1.1 m |

## 附錄 C：模糊隸屬函數圖

![隸屬函數](../src/fuzzy_control_design/fuzzy_design_v2.1/fuzzy_membership_functions_v2.1.png)

## 附錄 D：實驗軌跡圖

### 成功避障軌跡
![成功避障](../../rf2o_laser_odometry/bagfiles/results/trajectory_20260108_233345.png)

### LiDAR 碰撞 vs 視覺避障比較
![比較](../../rf2o_laser_odometry/bagfiles/results_crash/crash_trajectory_20260108_232817.png)

---

> **報告結束**
